{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to PyTorch for NLP1 (2018)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntokoven/LearningX/blob/master/Introduction_to_PyTorch_for_NLP1_(2018).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yvHMf9kxpBlA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Introduction to PyTorch for NLP1\n",
        "==========================\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OwJB10hhpQFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook is meant to give a short introduction to PyTorch basics.\n",
        "\n",
        "If, before or during we dive into PyTorch, you would like to read a bit more about neural networks in general, then [have a look here](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/).\n",
        "\n",
        "\n",
        "**You do not have to hand in this tutorial.** It is just to help you get started with PyTorch and neural networks.\n",
        "\n",
        "Before you continue, check if you have the correct runtime:\n",
        "\n",
        "> `Runtime -> Change runtime type`\n",
        "\n",
        "It should be Python 3 with GPU.\n",
        "\n",
        "Before you continue, please make a copy of this notebook to make it editable.\n",
        "\n",
        "### Installing PyTorch\n",
        "\n",
        "Now, let's install PyTorch:"
      ]
    },
    {
      "metadata": {
        "id": "baaS2wdKpcbt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s8czvNTYpauF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# should say 0.4.1\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zUXA_dcxpBlF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Seed\n",
        "\n",
        "We can set a seed so that we get the same random values each time we re-run the notebook.\n",
        "We will use seed [42](https://goo.gl/S3wrAV) here. "
      ]
    },
    {
      "metadata": {
        "id": "S32k33BMpBlF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDx0JrAapBlK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the Torch equivalent of **Numpy arrays**, but crucially, they can also be used on a GPU (graphics card). This can make your calculations a lot faster.\n",
        "\n",
        "Since they are so similar, you can actually **convert** most tensors to Numpy arrays (and back), but we won't need to do that so often.\n",
        "\n",
        "Working with PyTorch, we will need lots of tensors of various shapes.\n",
        "For example, if we want to transform an input vector $\\mathbf{x}$, we will need a weight matrix $W$.\n",
        "\n",
        "**Note:** \"Tensor\" is a general name. A 1-D tensor is also called a **vector**, a 2-D tensor a **matrix**.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BGLXG2zxpBlL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create *uninitialized* 3-D tensor (values can be anything that is in memory!)\n",
        "x = torch.Tensor(2, 3, 3)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcPJIJH6pBlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# a randomly initialized 2-D tensor (a matrix)\n",
        "x = torch.rand(4, 3)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phKlnBBrpBlS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# how to get its size\n",
        "print(x.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qv4D6_K9pBlV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# or if you know there are 2 dimensions:\n",
        "time, dim = x.size()\n",
        "print(time, dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivHu1ZxupBla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Operations\n",
        "\n",
        "#### Adding"
      ]
    },
    {
      "metadata": {
        "id": "MTpc1haWpBlc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# You can add tensors in many ways. \n",
        "# The easiest is to simply use a python + operator:\n",
        "y = torch.rand(4, 3)\n",
        "print(x + y)\n",
        "\n",
        "# But you can also use torch.add:\n",
        "print(torch.add(x, y))\n",
        "\n",
        "# Provide an output Tensor and save the result there:\n",
        "result = torch.Tensor(4, 3)\n",
        "torch.add(x, y, out=result)\n",
        "print(result)\n",
        "\n",
        "# Or add in-place (this changes y!)\n",
        "# Note: Any operation that mutates a tensor in-place is post-fixed with an \"_\", like \"add_\" here.\n",
        "y.add_(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uoOWYbIypBlg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Matrix multiplication\n",
        "\n",
        "Matrix multiplications are essential for Neural networks. Quite often, we have an input vector $\\mathbf{x}$, and then we want to learn weights $W$ that transform that input to some output that we want. \n",
        "\n",
        "We will now walk you through matrix multiplication in PyTorch."
      ]
    },
    {
      "metadata": {
        "id": "-ZpRuE8cpBlh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's create a vector x with values 0..5\n",
        "# We can use the arange function for that:\n",
        "x = torch.arange(0, 6)\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xw-AzLnqpBlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now, we will reshape x to have shape 2x3\n",
        "# That is, it will become a matrix!\n",
        "# The values will be the same, we will just look at them differently.\n",
        "x = x.view((2, 3))\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMTx-MmFpBlo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now, let's create a square matrix W:\n",
        "W = torch.arange(0, 9).view((3, 3))\n",
        "print(W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "B8hsX8x4pBlr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now we can perform matrix multiplication, since we have 2x3 and 3x3 matrices!\n",
        "# Verify if you can do this multiplication by hand, too!\n",
        "# If you need some help, you can check here: https://www.mathsisfun.com/algebra/matrix-multiplying.html\n",
        "h = torch.matmul(x, W)\n",
        "print(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gdJfT0F6pBlt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### More operations\n",
        "In case you want to do something different from addition or matrix-multiplying (and that is quite likely!), you can read here about all of Torch's operations: https://pytorch.org/docs/stable/index.html\n"
      ]
    },
    {
      "metadata": {
        "id": "F9LMraJcpBlu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Indexing\n",
        "It is quite common that we need to select a part of a tensor. Indexing works just like in Numpy!"
      ]
    },
    {
      "metadata": {
        "id": "yMpHi0VopBlu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(result[:, 1])    # second column\n",
        "print(result[0])       # first row\n",
        "print(result[:2, -1])  # first two rows, last column\n",
        "print(result[1:3, :])  # middle two rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MeWjjBvGpBlx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Automatic differentiation with Autograd\n",
        "\n",
        "One of the main reasons for using PyTorch is that we can automatically get **gradients/derivatives** of functions that we define. We will mainly use PyTorch for using Neural networks, and they are just fancy functions! If we use weight matrices in our function that we want to learn, then those are called the **parameters** or simply the **weights**.\n",
        "\n",
        "If our Neural Network would output a single scalar value, we would talk about taking the **derivative**, but you will see that quite often we will have **multiple** output variables (\"values\"); in that case we talk about **gradients**. It's a more general term.\n",
        "\n",
        "Given an input $\\mathbf{x}$, we define our function by **manipulating** that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a **computational graph**. This graph shows how to arrive at our output from our input. \n",
        "PyTorch is a **define-by-run** framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us!\n",
        "\n",
        "So, to recap: the only thing we have to do is to compute the **output**, and then we can ask PyTorch to automatically get the **gradients**. \n",
        "\n",
        "> **Note:  Why do we want gradients?** Consider that we have defined a function, a Neural Net, that is supposed to compute a certain output $y$ for an input vector $\\mathbf{x}$. We then define an **error measure** that tells us how wrong our network is; how bad it is in predicting output $y$ from input $\\mathbf{x}$. Based on this error measure, we can use the gradients to **update** the weights $W$ that were responsible for the output, so that the next time we present input $\\mathbf{x}$ to our network, the output will be closer to what we want. "
      ]
    },
    {
      "metadata": {
        "id": "O-a89JqW9le7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Enabling automatic gradients\n",
        "\n",
        "In order to get the autograd functionality, a tensor needs to set `requires_grad` to `True`. This is set to False by default.\n"
      ]
    },
    {
      "metadata": {
        "id": "pFZnrImI9oOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = torch.ones(3)\n",
        "t.requires_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3c4aiVO-cbN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = torch.ones(3, requires_grad=True)\n",
        "t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yopHsYC89iwM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Example\n",
        "\n",
        "We're going to define a function $$y_i = (x_i + 2)^2 + 3$$\n",
        "And as our final output $o$ we take the mean over all values $y_i$, so we get a single output value:\n",
        "\n",
        "$$o = \\frac{1}{|y|} \\sum_i y_i$$\n",
        "\n",
        "As our input $\\mathbf{x}$ we'll use a vector with 3 values: $[1, 1, 1]$."
      ]
    },
    {
      "metadata": {
        "id": "XREE1B3VpBlx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create an input vector x\n",
        "x = torch.ones(3, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cgtmRfXcpBl0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now we define our function\n",
        "# Note that, even though x is a vector, we can still add a single value to it.\n",
        "# PyTorch will just add that value to each element of the vector.\n",
        "y = (x + 2)**2 + 3\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UofeoWLjpBl2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# y has a grad_fn since it was created by an operation\n",
        "# this grad_fn will be used by PyTorch for obtaining the gradient\n",
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0fW9-MnkpBl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our final output o is the mean\n",
        "o = y.mean()\n",
        "print(o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UvtX8gp6pBl7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now we can take the gradients by calling o.backward()\n",
        "# this will populate x.grad\n",
        "o.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2WIyY-zpBl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`x.grad` will now contain the gradient $\\partial o/ \\partial x$, and this will say how a change in $x$ will affect output $o$:"
      ]
    },
    {
      "metadata": {
        "id": "xp0FHgdrpBl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNorbxodpBl_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Verifying the gradient by hand\n",
        "\n",
        "You should now see a gradient of `[2, 2, 2]`.\n",
        "\n",
        "We can verify this by hand!\n",
        "\n",
        "Our output $\\mathbf{o}$ is the mean of $\\mathbf{y}$:\n",
        "$$o = \\frac{1}{3}\\sum_i y_i$$\n",
        "\n",
        "And $\\mathbf{y}$ consists of elements $y_i$:\n",
        "\n",
        "$$y_i = (x_i+2)^2 + 3$$ \n",
        "\n",
        "We know that $y_i = 12$, given that $x_i = 1$ (for each $i$):\n",
        "$$y_i\\bigr\\rvert_{x_i=1} = 12$$\n",
        "\n",
        "Therefore,\n",
        "$$\\frac{\\partial o}{\\partial x_i} = \\frac{\\partial o}{\\partial y_i}\\frac{\\partial y_i}{\\partial x_i} = \\underbrace{\\frac{1}{3}}_{\\frac{\\partial o}{\\partial y_i}} \\underbrace{2 (x_i+2)}_{\\frac{\\partial y_i}{\\partial x_i}} = \\frac{2}{3} (x_i+2)$$\n",
        "\n",
        "hence\n",
        "$$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{2}{3} * 3 = 2$$"
      ]
    },
    {
      "metadata": {
        "id": "-UvOaiyTpBmB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## NN module\n",
        "\n",
        "Now if we want to build a big Neural Net, we could specify all our parameters (weight matrices, bias vectors) using `Tensors` (with `requires_grad=True`), ask PyTorch to calculate the gradients and then adjust the parameters. But things can quickly get cumbersome if we have a lot of parameters. In PyTorch, there is a package called `torch.nn` that makes building Neural Nets more convenient. \n",
        "\n",
        "Let's define a very simple Neural Net to show you how it works. The network performs a **logistic regression**, i.e. it calculates:\n",
        "$$ y = \\sigma( W \\mathbf{x} + b )$$\n",
        "\n",
        "You have already seen how to calculate $W \\mathbf{x} + b$; it's a matrix multiplication with an added bias. The function $\\sigma$ might be new: it is the sigmoid function, and it is defined as:\n",
        "$$ \\sigma(x) = \\frac{1}{1+ \\exp(-x)} $$\n",
        "\n",
        "The $\\exp$ makes sure all values are positive, while the rest scales them between 0 and 1. \n",
        "You can see the function below:"
      ]
    },
    {
      "metadata": {
        "id": "PjUzy4xspBmB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "x = np.arange(-10., 10., 0.2)\n",
        "plt.plot(x, sigmoid(x), 'b', label='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8CTVNKmgBeCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Tip:** if you want to apply a sigmoid fuction to a Tensor `x` in Pytorch, you can simply call `x.sigmoid()`."
      ]
    },
    {
      "metadata": {
        "id": "FuzxBax0pBmF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In NLP, we are often doing **classification**, rather than regression (predicting a value). So, even though the name can be misleading, logistic regression is a **classifier**: we have two output classes 0 and 1.\n",
        "\n",
        "To get the 2 classes, we use the sigmoid function $\\sigma$, so that the values coming out of our NN are between 0 and 1. You can see that in the picture above.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "j808UwLZpBmF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ExampleNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ExampleNN, self).__init__()\n",
        "        \n",
        "        # an affine operation: y = xW + b\n",
        "        self.output_layer = nn.Linear(3, 1, bias=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.output_layer(x)\n",
        "        return x.sigmoid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nbr-Z02dKb9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a model instance\n",
        "torch.manual_seed(42)\n",
        "model = ExampleNN()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9T8y0_8epBmJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, this is a very simple Neural Network! Actually, it is so simple maybe we should not call it a Neural network. But let's do so anyway.\n",
        "\n",
        "This is what you should know: \n",
        "\n",
        "- when defining your Neural Net, you create a class that *inherits* from `nn.Module`. \n",
        "- We called our Neural Net `ExampleNN`.\n",
        "- Parameters are defined within the `__init__` method. Here we defined a single **linear** layer. The parameters for that layer (a weight matrix $W$ and a bias term $b$) are added **automatically** to our parameter list.\n",
        "- We define our computation in the `forward` function. In this case, we apply the linear layer to our input $\\mathbf{x}$ and then a sigmoid function.\n",
        "\n",
        "Let's check if our parameters are indeed as we expect them to be:"
      ]
    },
    {
      "metadata": {
        "id": "Zkh47ogzpBmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = list(model.named_parameters())\n",
        "\n",
        "for name, p in params:\n",
        "    print(name, p)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0rSy2VppBmO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see that:\n",
        "\n",
        "- the first parameter is our weight matrix $W$, which is shaped $[1, 3]$.\n",
        "- the second parameter is the bias term $b$, which is a scalar (a single value), since the weight matrix transforms our input $\\mathbf{x}$ (with 3 elements) into a single scalar.\n",
        "\n",
        "Observe that these parameters have been randomly initialized.\n",
        "\n",
        "Now that we have our NN, we can feed it an input and see what comes out.\n",
        "The input has to be a `Variable`. "
      ]
    },
    {
      "metadata": {
        "id": "Z4U3Y8LWpBmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(\"input:\", x)\n",
        "out = model(x)\n",
        "print(\"output:\", out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wi8AcrvxpBmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Loss \n",
        "Now we would like to use the `autograd` functionality to get gradients, but we first need a loss!\n",
        "The loss will tell us how well our network is doing.\n",
        "\n",
        "We're going to say that, for our input example, the **target** value is `0`. \n",
        "The target is what we wanted our network to **predict** for the input that we gave.\n",
        "\n",
        "As our **loss** (or \"criterion\") we'll use the following two options:\n",
        "\n",
        "##### Mean Squared Error\n",
        "\n",
        "Mean Squared Error (MSE) is given by:\n",
        "\n",
        "$$ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (o_i - t_i)^2 $$\n",
        "\n",
        "I.e. it is the average, over elements $i$, of the squared difference of output $o_i$ with target $t_i$.\n",
        "Since we have a single output value here, our loss is simply $(o - t)^2$.\n",
        "\n",
        "##### Cross Entropy Loss\n",
        "\n",
        "As there are only two output classes, we can use Binary Cross Entropy (BCE) Loss, which is given by:\n",
        "\n",
        "$$ \\text{BCE} = \\frac{1}{N} \\sum_{i=1}^N −(t_i \\log(o_i)+(1−t_i)\\log(1−o_i)) $$\n",
        "\n",
        "BCE averages a different measure of the divergence between outputs and targets. For an individual sample $i$, it takes the following value:\n",
        "\n",
        "$$\n",
        "\\displaystyle −(t_i \\log(o_i)+(1−t_i)\\log(1−o_i))  = \\begin{cases} \n",
        " −\\log(1 - o_i) & \\text{if $t_i = 0$}\\\\\n",
        " −\\log(o_i) & \\text{if $t_i = 1$} \\\\\n",
        " \\end{cases}\n",
        " $$\n",
        " \n",
        "Plotting these functions shows that Cross Entropy assigns a relatively high penalty to predictions that are very far from the target, while being more lenient to predictions that approach it:"
      ]
    },
    {
      "metadata": {
        "id": "sTsFglMI583G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "bce_0 = lambda x: - np.log(1 - x)\n",
        "bce_1 = lambda x: - np.log(x)\n",
        "x_array = np.arange(0.01, 1.0, 0.01)\n",
        "plt.plot(x_array, bce_0(x_array), 'b', label='t_i=0')\n",
        "plt.plot(x_array, bce_1(x_array), 'r', label='t_i=1')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tPE8IV5h7DUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch offers Mean Squared Error and Cross Entropy (in addition to many other options) as built-in loss functions. \n",
        "\n",
        "**Let's calculate our loss:**"
      ]
    },
    {
      "metadata": {
        "id": "HiSdEUbFpBmU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out = model(x)                # forward pass\n",
        "target = torch.zeros(1)       # a dummy target (0.)\n",
        "\n",
        "criterion_mse = nn.MSELoss()  # this is our MSE criterion\n",
        "criterion_bce = nn.BCELoss()  # this is our Binary Cross Entropy criterion\n",
        "\n",
        "loss_mse = criterion_mse(out, target)\n",
        "loss_bce = criterion_bce(out, target)\n",
        "\n",
        "print(\"output:\", out)\n",
        "print(\"MSE loss:\", loss_mse)\n",
        "print(\"BCE loss:\", loss_bce)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h1yrp29npBma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent \n",
        "Now we'll ask PyTorch to **update** the weights (parameters) of our neural network so that our next prediction is closer to that target.\n",
        "\n",
        "We first need to zero-out all gradient tensors. `net.zero_grad()` will do this for all parameters. It will set `p.grad` to zeros for each parameter $p$.\n",
        "\n",
        "We use MSE loss here, but you can try with BCE (or any other loss function) yourself. "
      ]
    },
    {
      "metadata": {
        "id": "CobZdcAppBma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.zero_grad()    # reset gradients\n",
        "loss_mse.backward()  # compute gradients\n",
        "\n",
        "# update weights\n",
        "learning_rate = 0.5\n",
        "\n",
        "# for each parameter, take a small step in the opposite dir of the gradient\n",
        "for p in model.parameters():\n",
        "    p.data = p.data - p.grad.data * learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zBoym17GpBmc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, if we check the output for the same input vector $\\mathbf{x}$, the output should be closer to the target:"
      ]
    },
    {
      "metadata": {
        "id": "pGOK3OgBpBmc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_out = model(x)\n",
        "new_loss_mse = criterion_mse(new_out, target)\n",
        "\n",
        "print(\"target:\", target)\n",
        "print(\"out:\", out)\n",
        "print(\"new out (should be closer to target):\", new_out)\n",
        "\n",
        "print(\"\\nloss:\", loss_mse)\n",
        "print(\"new loss (should be lower):\", new_loss_mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RD63c9YVzNJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using torch.optim\n",
        "\n",
        "We just did a step of gradient descent manually, but in practice we would use an optimizer provided by PyTorch, from [torch.optim](https://pytorch.org/docs/stable/optim.html). The simplest one does exactly the same as what we just did.\n",
        "\n",
        "Let's do the same update again, but now using the built-in SGD from PyTorch:\n"
      ]
    },
    {
      "metadata": {
        "id": "ATz7jXYY0BdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# create a new model with the same parameters as before\n",
        "# (because we use the same random seed)\n",
        "torch.manual_seed(42)\n",
        "model = ExampleNN()\n",
        "print(model)\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "# forward pass\n",
        "new_out = model(x)\n",
        "\n",
        "# loss\n",
        "new_loss_mse = criterion_mse(new_out, target)\n",
        "\n",
        "# backward pass\n",
        "model.zero_grad()    # reset gradients\n",
        "new_loss_mse.backward()  # compute gradients\n",
        "\n",
        "# adjust weights using the optimizer\n",
        "optimizer.step()\n",
        "\n",
        "# compute new output after updating\n",
        "new_out = model(x)\n",
        "new_loss_mse = criterion_mse(new_out, target)\n",
        "\n",
        "print(\"target:\", target)\n",
        "print(\"out:\", out)\n",
        "print(\"new out (should be closer to target):\", new_out)\n",
        "\n",
        "print(\"\\nloss:\", loss_mse)\n",
        "print(\"new loss (should be lower):\", new_loss_mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NQtWT7AKJLJU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Observe** that using optim.SGD for 1 step had exactly the same effect as our manual update."
      ]
    },
    {
      "metadata": {
        "id": "rbqvEq-v2G_-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Final notes\n",
        "\n",
        "**Final note on optimizers**\n",
        "\n",
        "The advantage of using an optimizer from `torch.optim` is that we can drop-in replace it with another, better optimizer that converges faster. We will do so in the practical. Just know that, in essence, everything it does comes down to taking a step in the opposite direction of the gradient, just like you just did manually!\n",
        "\n",
        "**Final note on loss functions**\n",
        "\n",
        "In this tutorial we played with 2 losses: MSE and Binary Cross Entropy. \n",
        "In the practical, our set of outputs will be 5 sentiment classes. We could map these to a value between 0 and 1 and use MSE, or (and this is what we will do), use a Cross Entropy Loss, with 5 classes. We will cover this loss in the lectures."
      ]
    },
    {
      "metadata": {
        "id": "sUrrPt_kuTAe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This is it! You are now ready for Practical II."
      ]
    }
  ]
}